{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d33SKqwSqTdb",
        "outputId": "6a94fea2-3142-4e16-9b1a-94cb5386da59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Import google drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2kZscknpUdz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAKZJfAfpOT1"
      },
      "outputs": [],
      "source": [
        "#Step 1: Preparing Dataset\n",
        "images_dir = '/content/drive/My Drive/images'\n",
        "if not os.path.exists(images_dir):\n",
        "    os.makedirs(images_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcKGNqZ1puXD"
      },
      "outputs": [],
      "source": [
        "# Creating training, validation and test directories\n",
        "base_dir = '/content/drive/My Drive/splits'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "val_dir = os.path.join(base_dir, 'validation')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# Creating subdirectories for each category\n",
        "for split_dir in [train_dir, val_dir, test_dir]:\n",
        "    for category in ['cars', 'bicycles', 'mountains', 'deer']:\n",
        "        os.makedirs(os.path.join(split_dir, category), exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bt2BWGE_p2QC",
        "outputId": "3c3280fd-0e6f-4373-ab42-65b6c70bd941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset successfully split into training, validation, and test sets!\n"
          ]
        }
      ],
      "source": [
        "# Spliting dataset into training, validation and test - 70%, 20% and 10% respectively\n",
        "for category in ['cars', 'bicycles', 'mountains', 'deer']:\n",
        "    category_dir = os.path.join(images_dir, category)\n",
        "\n",
        "    if not os.path.exists(category_dir):\n",
        "        print(f\"Warning: Category directory not found: {category_dir}\")\n",
        "        continue\n",
        "\n",
        "    images = os.listdir(category_dir)\n",
        "\n",
        "    train_images, temp_images = train_test_split(images, test_size=0.3, random_state=42)\n",
        "    val_images, test_images = train_test_split(temp_images, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Moving images to the directories where they belong\n",
        "    for image in train_images:\n",
        "        source_path = os.path.join(category_dir, image)\n",
        "        destination_path = os.path.join(train_dir, category, image)\n",
        "\n",
        "        if os.path.exists(source_path):\n",
        "            shutil.copy(source_path, destination_path)\n",
        "        else:\n",
        "            print(f\"Warning: Image file not found: {source_path}\")\n",
        "\n",
        "    for image in val_images:\n",
        "        shutil.copy(os.path.join(category_dir, image), os.path.join(val_dir, category, image))\n",
        "    for image in test_images:\n",
        "        shutil.copy(os.path.join(category_dir, image), os.path.join(test_dir, category, image))\n",
        "\n",
        "print(\"Dataset successfully split into training, validation, and test sets!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXGPbIANMBNm",
        "outputId": "6faa753d-301d-4fd7-a679-a9223853e754"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleting corrupt image: /content/drive/My Drive/splits/train/deer/Nagano's_deer_(49785030382).jpg\n",
            "Deleting corrupt image: /content/drive/My Drive/splits/train/deer/.DS_Store\n",
            "All corrupt images removed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Defining the function that deletes corrupt images\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def delete_corrupt_images(directory):\n",
        "    for subdir, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(subdir, file)\n",
        "            try:\n",
        "                with Image.open(file_path) as img:\n",
        "                    img.convert(\"RGB\").save(file_path)\n",
        "            except (OSError, IOError):\n",
        "                print(f\"Deleting corrupt image: {file_path}\")\n",
        "                os.remove(file_path)\n",
        "\n",
        "# Apply to all dataset folders\n",
        "for folder in [train_dir, val_dir, test_dir]:\n",
        "    delete_corrupt_images(folder)\n",
        "\n",
        "print(\"All corrupt images removed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTxIvOLjTGma",
        "outputId": "3db262f6-6ec2-4dc3-bb28-3aa0dd8c938b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1065 images belonging to 4 classes.\n",
            "Found 304 images belonging to 4 classes.\n",
            "Found 154 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Data Augmentation and Generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_data = val_test_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_data = val_test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjPbhQOrTNsp",
        "outputId": "4a3c39d4-fa10-4552-ccd5-144aca4b16e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Model Design\n",
        "# Using a CNN (Convolutional Neural Network) because of its effectiveness. Less parameters required, pooling layers and handles colour channels and depth well.\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compiling the Model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYR2wA36TQnq",
        "outputId": "209a12e9-e666-462c-88df-463ae14eca79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 9s/step - accuracy: 0.7246 - loss: 0.7144 - val_accuracy: 0.7039 - val_loss: 0.8542\n",
            "Epoch 2/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 9s/step - accuracy: 0.6946 - loss: 0.7987 - val_accuracy: 0.7467 - val_loss: 0.7337\n",
            "Epoch 3/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 8s/step - accuracy: 0.7075 - loss: 0.7433 - val_accuracy: 0.7138 - val_loss: 0.8099\n",
            "Epoch 4/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 9s/step - accuracy: 0.7028 - loss: 0.7656 - val_accuracy: 0.6480 - val_loss: 0.8935\n",
            "Epoch 5/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 9s/step - accuracy: 0.7268 - loss: 0.7542 - val_accuracy: 0.6579 - val_loss: 0.9453\n",
            "Epoch 6/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 9s/step - accuracy: 0.7097 - loss: 0.7464 - val_accuracy: 0.7105 - val_loss: 0.8168\n",
            "Epoch 7/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 9s/step - accuracy: 0.7052 - loss: 0.7415 - val_accuracy: 0.7664 - val_loss: 0.6909\n",
            "Epoch 8/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 9s/step - accuracy: 0.7277 - loss: 0.6982 - val_accuracy: 0.6711 - val_loss: 1.0793\n",
            "Epoch 9/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 9s/step - accuracy: 0.7195 - loss: 0.7323 - val_accuracy: 0.7105 - val_loss: 0.8552\n",
            "Epoch 10/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 10s/step - accuracy: 0.7008 - loss: 0.7604 - val_accuracy: 0.7303 - val_loss: 0.8500\n",
            "Epoch 11/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 8s/step - accuracy: 0.7202 - loss: 0.6813 - val_accuracy: 0.7533 - val_loss: 0.6808\n",
            "Epoch 12/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 9s/step - accuracy: 0.7326 - loss: 0.6640 - val_accuracy: 0.7336 - val_loss: 0.7713\n",
            "Epoch 13/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 9s/step - accuracy: 0.7442 - loss: 0.7098 - val_accuracy: 0.7204 - val_loss: 0.8008\n",
            "Epoch 14/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 9s/step - accuracy: 0.7385 - loss: 0.6983 - val_accuracy: 0.7401 - val_loss: 0.7041\n",
            "Epoch 15/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 8s/step - accuracy: 0.7591 - loss: 0.6493 - val_accuracy: 0.6875 - val_loss: 0.9384\n",
            "Epoch 16/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 9s/step - accuracy: 0.7333 - loss: 0.7132 - val_accuracy: 0.7105 - val_loss: 0.9234\n",
            "Epoch 17/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 8s/step - accuracy: 0.7494 - loss: 0.6473 - val_accuracy: 0.6941 - val_loss: 1.0449\n",
            "Epoch 18/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 9s/step - accuracy: 0.7661 - loss: 0.6115 - val_accuracy: 0.7237 - val_loss: 0.8317\n",
            "Epoch 19/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 8s/step - accuracy: 0.7831 - loss: 0.6131 - val_accuracy: 0.7368 - val_loss: 0.8054\n",
            "Epoch 20/20\n",
            "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 8s/step - accuracy: 0.7665 - loss: 0.5887 - val_accuracy: 0.7500 - val_loss: 0.7819\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Model Training\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,\n",
        "    epochs=20,\n",
        "    batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awj-LwI7R3-L",
        "outputId": "1694494a-1abc-4f84-e00d-42a9ba0a8bef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5s/step - accuracy: 0.7416 - loss: 0.7652\n",
            "Test Accuracy: 73.38%\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Model Evaluation\n",
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edNQId-lTTaV",
        "outputId": "70ab3242-c5fb-4751-bdf8-b4e5ea1c28b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# Step 6: Save the Model\n",
        "model.save('image_classifier.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhNhWODfqKrY"
      },
      "outputs": [],
      "source": [
        "# Step 7: Define the function to classify new images\n",
        "def classify_new_image(img_path, tf_model, class_names):\n",
        "\n",
        "    # Loading and preprocessing the image\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = img_array / 255.0\n",
        "\n",
        "    # Predicting the class\n",
        "    predictions = tf_model.predict(img_array)\n",
        "    predicted_class_index = np.argmax(predictions, axis=1)[0]\n",
        "    predicted_class_name = class_names[predicted_class_index]\n",
        "\n",
        "    # Displaying the image and prediction\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Predicted: {predicted_class_name}\")\n",
        "    plt.show()\n",
        "\n",
        "    return predicted_class_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cNHXuCMqM5Z",
        "outputId": "46017152-1952-4ad5-a6a5-c6084585f5f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "# Loading the saved model\n",
        "loaded_model = tf.keras.models.load_model('image_classifier.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ort14XE3_-pd",
        "outputId": "6eac6b42-80c4-4fe8-e1fb-babb30d5f537"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to /content/drive/My Drive/image_classifier.h5\n"
          ]
        }
      ],
      "source": [
        "# Define model path in Google Drive\n",
        "drive_model_path = '/content/drive/My Drive/image_classifier.h5'\n",
        "\n",
        "# Save the model\n",
        "loaded_model.save('image_classifier.h5')\n",
        "\n",
        "# Copy model to Google Drive\n",
        "shutil.copy('image_classifier.h5', drive_model_path)\n",
        "\n",
        "print(f\"Model saved to {drive_model_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
